#
# Date created: 2022-07-05
# Author: aradclif
#
#
############################################################################################
# https://www.jstatsoft.org/article/view/v011i03
#### Marsaglia's Square Histogram (Method II in the above article)
# p ‚àà ‚Ñù·¥∫, ‚àë·µ¢p·µ¢ = 1, a = 1/N
# K ‚àà ‚Ñï·¥∫, K·µ¢ = i
# V ‚àà ‚Ñù‚Åø, V·µ¢ = i * a
# Generate: j = ‚åäN*U+1‚åã; if U < V[j], return j, else return K[j]
# Theoretically, just one U ~ Uniform(0,1) is sufficient. In practice, it is also faster as
#     U=rand(); j=floor(Int, N * U + 1)
# is far fewer instructions than
#     j=rand(1:N); U=rand()
#### Robin Hood
## Motivation
# The frequency of the `else` statement being required is proportional to the "over-area",
# i.e. the part of the final squared histogram that lies above the division points.
# Or, to quantify it: ‚àë·µ¢ (i * a) - V[i]
# The objective is to minimize the number of times the `else return K[j]` occurs
# by creating V such that each V[j] is as close to j/N as possible -- an NP-hard problem.
# Marsaglia's suggestion of the Robin Hood method is a good solution which is ùí™(NlogN).
## Thoughts
# The reduction in `else` statements leads to faster sampling -- ùí™(1) regardless --
# as it would certainly lead to a more predictable instruction pipeline due to minimizing
# the number of times the `else` branch is executed (or used, even if executed).
# Does it justify the ùí™(NlogN) construction cost for the alias tables? -- given
# that we could pay ùí™(N) instead to construct an inferior table (but no limit on how terrible).
#     - An analysis based on the objective function above would be informative;
#       can be verified with Monte Carlo: compute ùíª(ùê±) = ‚àë·µ¢ (i * a) - V[i]
#       using the V produced by each procedure.
#     - Number of samples to be drawn is an orthogonal decision variable;
#       one surmises that increasing number of samples favor better table.
## Algorithm
# repeat these two steps N - 1 times
#     1. find the smallest probability, p·µ¢, and the largest probability, p‚±º
#     2. set K[i] = j; V[i] = (i - 1) * a + p·µ¢; replace p‚±º with p‚±º - (a - p·µ¢)
## Numerical stability
# Replacing p‚±º is the only point at which stability is a real concern. There are a few
# options for the order of operations and parenthesis. Unsurprisingly, Marsaglia gives
# the most stable form: p‚±º = p‚±º - (a - p·µ¢)
# But it is worthwhile to show that this is the most stable form.
#     First, consider that it should be the case that p·µ¢ ‚â§ a, hence 0 ‚â§ (a - p·µ¢) ‚â§ 1/n.
#     (a - p·µ¢) may be a small number, but (a - p·µ¢) is always well-defined since it occurs
#     at eps(a)‚â°ulp(a).
# It is worth noting that the (a - p·µ¢) operation becomes unstable when p·µ¢ ‚â§ eps(a)/4, assuming
# the worst case, a=1. However, this has the opposite relationship to n: increasing n will
# result in a subtraction which takes place at smaller values, hence, the (floating)
# points are more densely packed (i.e. distance to nearest float is smaller).
# It is reassuring to note that eps(.5) = 2‚Åª‚Åµ¬≥, hence, even for a vector of length 2,
# the (a - p·µ¢) is stable for p·µ¢ > 2‚Åª‚Åµ‚Åµ.
#     The subsequent subtraction, i.e. p‚±º - (a - p·µ¢), will occur at eps(p‚±º)‚â°ulp(p‚±º). Thus,
#     the operation will be unstable when p‚±º - c * ulp(p‚±º), c ‚â§ 1/4 (for p‚±º = 1, the worst case).
# That is, unstable when: (a - p·µ¢) ‚â§ eps(p‚±º)/4
# If p·µ¢ ‚âà 0, (a - p·µ¢) ‚âà 1/n ‚üπ 1/n ‚â§ eps(p‚±º)/4 is unstable
# As p‚±º is at most 1, the worst case will be eps(p‚±º)/4 = 2‚Åª‚Åµ‚Å¥, i.e. 1/n ‚â§ 2‚Åª‚Åµ‚Å¥.
# ‚à¥ in the worse case, instability begins at n ‚â• 2‚Åµ‚Å¥ if p·µ¢ ‚âà 0.
# ‚à¥ in general, expect instability if (a - p·µ¢) ‚â§ 2‚Åª‚Åµ‚Å¥.
# The above assumed Float64 with 53 bits of precision; the general form in terms of precision
# replaces 2‚Åª‚Åµ‚Å¥ ‚Üí 2‚Åª·µñ‚Åª¬π.
# These are very permissive bounds; one is likely to run into other issues well before
# the algorithm becomes numerically unstable.
## Numerical stability, revisit
# Oddly, Marsaglia's implementation in TplusSQ.c uses p‚±º + p·µ¢ - a, which has slightly
# worse numerical stability. (p‚±º + p·µ¢) becomes unstable when p·µ¢ ‚â§ eps(p‚±º)/2, which
# corresponds to 2‚Åª·µñ at p‚±º = 1.
# It is possible to find cases where both suffer roundoff, which is ‚â§ 2‚Åª·µñ‚Å∫¬π for p‚±º + p·µ¢ - a
# and ‚â§ 2‚Åª·µñ for p‚±º - (a - p·µ¢).
# Provided that one is working with Float64, it most likely does not matter.
# However, if p is provided as Float32, it may be preferable to forcibly promote to Float64
# just to ensure stability; naturally, Float16 is largely unsuitable and needs promotion.
# ##
# # Comparison of stability
# # f1 is unstable at n = 10, q‚±º = .999999999999
# # However, f2 and f3 are unstable at n = 10, q‚±º = 5
# f1(q‚±º, q·µ¢, a) = q‚±º - (a - q·µ¢)
# f2(q‚±º, q·µ¢, a) = q‚±º + q·µ¢ - a
# ff2(q‚±º, q·µ¢, a) = @fastmath q‚±º + q·µ¢ - a
# f3(q‚±º, q·µ¢, a) = (q‚±º + q·µ¢) - a
# n = 10
# a = 1 / n
# q‚±º = .999999999999
# q·µ¢ = (1 - q‚±º) / n
# f1(q‚±º, q·µ¢, a)
# f2(q‚±º, q·µ¢, a)
# ff2(q‚±º, q·µ¢, a)
# f3(q‚±º, q·µ¢, a)
# f1(big(q‚±º), big(q·µ¢), big(a))
# f2(big(q‚±º), big(q·µ¢), big(a))
# f3(big(q‚±º), big(q·µ¢), big(a))
# # Another example
# f1(1.0, eps()/2, .0003)
# f2(1.0, eps()/2, .0003)
# f2(1.0, big(eps()/2), .0003)
# isfeq(q‚±º, q·µ¢, a) = f1(q‚±º, q·µ¢, a) == f2(q‚±º, q·µ¢, a)
# isfeq_big(q‚±º, q·µ¢, a) = f1(q‚±º, q·µ¢, a) == Float64(f2(big(q‚±º), big(q·µ¢), big(a)))
# count(i -> isfeq(q‚±º, i, a), q·µ¢:q·µ¢:n*q·µ¢)
# isfeq.(q‚±º, q·µ¢:q·µ¢:n*q·µ¢, a)
# isfeq_big.(q‚±º, q·µ¢:q·µ¢:n*q·µ¢, a)


